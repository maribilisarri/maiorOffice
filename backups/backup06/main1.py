import ast
import shutil
import pandas as pd
import os
import random
from datetime import datetime, timedelta
import re
import glob
import logging
from collections import defaultdict

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
#-----------------Paths------------------------
input_dir='input'
static_dir = 'static'
timetable_dir='timetable-output'
blocks_dir="blocks-output"
duties_dir="duties-output"
history_dir='history'
#combined_output_path = 'output/combined_all_timetables.csv'
for d in (timetable_dir,blocks_dir,duties_dir,history_dir):
    os.makedirs(d, exist_ok=True)







# ----------- Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±: 'test' Î® 'final' -------------

while True:
    MODE = input("Î•Ï€Î¹Î»Î­Î¾Ï„Îµ mode ('test' Î® 'final'): ").strip().lower()
    if MODE in ('test', 'final'):
        break
    print("âŒ ÎœÎ· Î­Î³ÎºÏ…ÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î®.  'test' Î® 'final'")
final = (MODE == 'final')


# ----------- Î™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ IDs (Î¼ÏŒÎ½Î¿ ÏƒÎµ final mode) -------------
trip_history_file = os.path.join(history_dir, 'used_trip_ids.txt')
duty_history_file = os.path.join(history_dir, 'used_duty_ids.txt')
used_trip_ids = set()
used_duty_ids = set()

if MODE == 'final':
    if os.path.exists(trip_history_file):
        with open(trip_history_file, 'r') as f:
            for line in f:
                uid = line.strip()
                if uid:
                    used_trip_ids.add(uid)

    if os.path.exists(duty_history_file):
        with open(duty_history_file, 'r') as f:
            for line in f:
                uid = line.strip()
                if uid:
                    used_duty_ids.add(uid)

# ----------- Random seed ÏƒÎµ test mode -------------
if MODE == 'test':
    random.seed(12345)
    #--------------------------- ----------- Î’Î¿Î·Î¸Î·Ï„Î¹ÎºÎ­Ï‚ Î£Ï…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ -------------------------------------
# ----------- Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± Î±ÏƒÏ†Î±Î»Î­Ï‚ random ID -------------
def get_unique_9digit(used_ids_set):
    while True:
        new_id = str(random.randint(10**8, 10**9 - 1))
        if MODE == 'test' or new_id not in used_ids_set:
            if MODE == 'final':
                used_ids_set.add(new_id)
            return new_id

# ----------- Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï -------------
def save_history():
    if MODE != 'final':
        return
    with open(trip_history_file, 'w') as f:
        for tid in sorted(used_trip_ids):
            f.write(f"{tid}\n")
    with open(duty_history_file, 'w') as f:
        for did in sorted(used_duty_ids):
            f.write(f"{did}\n")



# Î‘Î½ Î· Ï„Î¹Î¼Î® Ï„Î·Ï‚ VEHBLOCK_ID Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Î¿Ï€Î¿Î¹Î¿Î´Î®Ï€Î¿Ï„Îµ Î³ÏÎ¬Î¼Î¼Î±, Î¸Î­Ï„Î¿Ï…Î¼Îµ '99'

def replace_if_contains_letter(value):
    if pd.isna(value):
        return value
    return re.sub(r'\D+', '99', str(value))


#==================================================================================TIME==================================================
#===========================================================================================================================================
def parse_extended_time(time_str):
    """
    Î Î±Î¯ÏÎ½ÎµÎ¹ string "HH:MM:SS" ÏŒÏ€Î¿Ï… HH Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ <24 Î® >=24.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î­Î½Î± datetime Î±Î½Ï„Î¹ÎºÎµÎ¯Î¼ÎµÎ½Î¿:
      - Î‘Î½ HH >= 24, Ï„Î¿Ï€Î¿Î¸ÎµÏ„ÎµÎ¯ Ï„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± ÏƒÎµ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î·Î¼Î­ÏÎ± (1900-01-02)
      - Î‘Î½ HH <  24, ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Î·Î¼Î­ÏÎ± (1900-01-01)
    Î‘Î½ Ï„Î¿ time_str Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î³Î¯Î½ÎµÎ¹ parse, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ pd.NaT.
    """
    try:
        s = str(time_str).strip()
        if ' ' in s:
            s = s.split(' ')[-1]  # ÎšÏÎ±Ï„Î¬ÎµÎ¹ Î¼ÏŒÎ½Î¿ Ï„Î·Î½ ÏÏÎ±
        if len(s.split(':')) == 2:
            s += ':00'

        h, m, s = map(int, s.split(':'))
        if h >= 24:
            # Ï€.Ï‡. "24:05:00" â†’ 1900-01-02 00:05:00
            return datetime(1900, 1, 2, h - 24, m, s)
        else:
            # Ï€.Ï‡. "23:50:00" â†’ 1900-01-01 23:50:00
            return datetime(1900, 1, 1, h, m, s)
    except:
        return pd.NaT

def average_time(t1, t2):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î¿Î½ Î¼Î­ÏƒÎ¿ Ï‡ÏÏŒÎ½Î¿ Î±Î½Î¬Î¼ÎµÏƒÎ± ÏƒÎµ Î´ÏÎ¿ "extended" time-strings.
    Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±:
      t1 = "23:40:00", t2 = "00:10:00"  â†’ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± "24:25:00"
      t1 = "24:01:00", t2 = "00:25:00"  â†’ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± "24:13:00"
    Î”Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±:
      1) parse ÎºÎ±Î¹ Ï„Ï‰Î½ Î´ÏÎ¿ ÏƒÎµ datetime Î¼Îµ parse_extended_time
      2) Î±Î½ dt2 < dt1, Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ ÏŒÏ„Î¹ Ï„Î¿ t2 Î±Î½Î®ÎºÎµÎ¹ ÏƒÏ„Î·Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î·Î¼Î­ÏÎ± â†’ dt2 += 1 day
      3) mid = dt1 + (dt2 - dt1) / 2
      4) Î±Î½ mid.day == 2, Ï„ÏŒÏ„Îµ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ hour + 24, Î±Î»Î»Î¹ÏÏ‚ hour ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¬
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ string "HH:MM:SS" ÏŒÏ€Î¿Ï… HH >= 24 ÎµÎ¬Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹.
    """
    dt1 = parse_extended_time(t1)
    dt2 = parse_extended_time(t2)
    if pd.isna(dt1) or pd.isna(dt2):
        # fallback Î±Î½ Î´ÎµÎ½ parse-Î±ÏÎµÎ¹
        return t1

    # Î‘Î½ Î· Î´ÎµÏÏ„ÎµÏÎ· ÏÏÎ± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ·, Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ 1 Î·Î¼Î­ÏÎ±
    if dt2 < dt1:
        dt2 += timedelta(days=1)
    mid = dt1 + (dt2 - dt1) / 2
    if mid.day == 2:
        h = mid.hour + 24
    else:
        h = mid.hour
    return f"{h:02}:{mid.minute:02}:00"

def fix_excel_datetime_to_extended_hour_format(value, threshold_hour=4):
    """
    Î Î±Î¯ÏÎ½ÎµÎ¹ value Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹:
      - pd.Timestamp Î® datetime
      - float/int (Excel time fraction, Ï€.Ï‡. 0.5 â†’ 12:00:00)
      - string:
         * "HH:MM" Î® "HH:MM:SS"
         * "1900-01-01 HH:MM:SS" Î® "1/1/1900 HH:MM:SS"
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ string "HH:MM:SS" ÏŒÏ€Î¿Ï…, Î±Î½ hour < threshold_hour (Ï€.Ï‡. 4),
    Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ +24 (extended format). Î‘Î½ Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î³Î¯Î½ÎµÎ¹ parse ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ 'NaT'.
    """
    try:
        # 1) Î‘Î½ ÎµÎ¯Î½Î±Î¹ Timestamp / datetime
        if isinstance(value, (pd.Timestamp, datetime)):
            hour   = value.hour
            minute = value.minute
            second = value.second

        # 2) Î‘Î½ ÎµÎ¯Î½Î±Î¹ float/int (Excel time fraction)
        elif isinstance(value, (float, int)):
            total_seconds = int(value * 24 * 3600)
            hour   = total_seconds // 3600
            minute = (total_seconds % 3600) // 60
            second = total_seconds % 60

        else:
            # 3) Î‘Î½ ÎµÎ¯Î½Î±Î¹ string
            s = str(value).strip()

            # 3Î±) Î‘Î½ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ "1900-01-01" Î® "1/1/1900", ÎºÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î¿ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹ Ï„Î·Ï‚ ÏÏÎ±Ï‚
            #if '1900-01-01' in s or '1/1/1900' in s or '1900-01-02' in s or '1900-01-03' in s or '1900-11-01' in s:

            if ' ' in s:
                s = s.split(' ')[-1]  # Ï„Î¿ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹ Ï„Î·Ï‚ ÏÏÎ±Ï‚
            if len(s.split(':')) == 2:
                s += ':00'
                t = datetime.strptime(s, "%H:%M:%S")
                hour   = t.hour
                minute = t.minute
                second = t.second
            else:
                # 3Î²) Î‘Î½ Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ prefix, Ï…Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ "HH:MM" Î® "HH:MM:SS"
                parts = s.split(':')
                if len(parts) == 2:
                    # Ï€.Ï‡. "0:00" â†’ "00:00:00"
                    s = f"{int(parts[0]):02}:{int(parts[1]):02}:00"
                t = datetime.strptime(s, "%H:%M:%S")
                hour   = t.hour
                minute = t.minute
                second = t.second

        # 4) Î‘Î½ hour < threshold_hour (Ï€.Ï‡. 4), Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ Î¼ÎµÏ„Î¬ Î¼ÎµÏƒÎ¬Î½Ï…Ï‡Ï„Î± â†’ hour += 24
        if hour < threshold_hour:
            hour += 24

        return f"{hour:02}:{minute:02}:{second:02}"

    except:
        return 'NaT'

def adjust_overlapping_times(df):
    """
    Î“Î¹Î± ÎºÎ¬Î¸Îµ VB_COMPANYCODE ÎµÎ»Î­Î³Ï‡ÎµÎ¹ Î±Î½ Ï„Î¿ Î¤Î•Î¡ÎœÎ‘ Ï„Î·Ï‚ Ï„ÏÎ­Ï‡Î¿Ï…ÏƒÎ±Ï‚
    ÎµÎ³Î³ÏÎ±Ï†Î®Ï‚ ÎµÎ¯Î½Î±Î¹ > Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î— Ï„Î·Ï‚ ÎµÏ€ÏŒÎ¼ÎµÎ½Î·Ï‚. Î‘Î½ Î½Î±Î¹, Î¼Î¹ÎºÏÎ±Î¯Î½ÎµÎ¹
    Ï„Î¿Î½ Ï„ÎµÏÎ¼Î±Ï„Î¹ÏƒÎ¼ÏŒ ÏÏƒÏ„Îµ Î½Î± ÎµÎ¯Î½Î±Î¹ 1 Î»ÎµÏ€Ï„ÏŒ Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î±Î½Î±Ï‡ÏÏÎ·ÏƒÎ·,
    Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï„Î¿ â€œextended formatâ€ Ï„Î·Ï‚ ÏÏÎ±Ï‚ (Ï€.Ï‡. â€œ24:01:00â€).
    """


    changes = 0
    try:
        #for (vb_code, direction), group in df.groupby(['VB_COMPANYCODE', 'DIRECTION']):
        for (vb_code), group in df.groupby(['VB_COMPANYCODE']):
            group = group.copy()
            # Î£Ï„Î®Î»Î· Î³Î¹Î± Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·
            group['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—_dt'] = group['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'].apply(parse_extended_time)
            group = group.sort_values(by='Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—_dt').drop(columns='Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—_dt')

            for i in range(len(group) - 1):
                idx_now  = group.index[i]
                idx_next = group.index[i + 1]

                t_end        = parse_extended_time(df.at[idx_now,  'Î¤Î•Î¡ÎœÎ‘'])
                t_next_start = parse_extended_time(df.at[idx_next, 'Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'])

                if pd.notna(t_end) and pd.notna(t_next_start) and t_end > t_next_start:
                    # ÎœÎµÎ¹ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ Ï„ÏÎ­Ï‡Î¿Î½ â€œÎ¤Î•Î¡ÎœÎ‘â€ ÎºÎ±Ï„Î¬ 1 Î»ÎµÏ€Ï„ÏŒ,
                    # Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï„Î¿ extended format:
                    #new_dt   = t_next_start - timedelta(minutes=1)
                    new_time = (t_next_start - timedelta(minutes=1)).strftime('%H:%M:%S')
                    df.at[idx_now, 'Î¤Î•Î¡ÎœÎ‘'] = new_time
                    changes += 1

            last_idx = group.index[-1]
#DEBUG#
            #print("âœ” Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î± ÎµÎ³Î³ÏÎ±Ï†Î® Î³Î¹Î±", vb_code, ":",
           #       df.loc[last_idx, ['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—', 'Î¤Î•Î¡ÎœÎ‘']])

        print(f"ğŸ”§ ÎˆÎ³Î¹Î½Î±Î½ {changes} Î±Î»Î»Î±Î³Î­Ï‚ overlapping ÏƒÏ„Î¹Ï‚ ÏÏÎµÏ‚ Î¤Î•Î¡ÎœÎ‘ Î±Î½Î¬ VB_COMPANYCODE")
    except Exception as e:
        print(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î· ÏÏÎ¸Î¼Î¹ÏƒÎ· overlapping: {e}")
    return df

#==================================================================================TIME==================================================
#===========================================================================================================================================

def assign_duty_ids(df):
    """
    Î Î±Î¯ÏÎ½ÎµÎ¹ DataFrame Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»Î· 'DUTY_COMPANYCODE' Îº.Î¬.
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Ï…Ï‡Î±Î¯Î¿ DUTY_ID Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î¿Î½Î±Î´Î¹ÎºÏŒ DUTY_COMPANYCODE Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ '_',
    ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Î½Ï„Î±Ï‚ Î½Î­Î¿ DataFrame Î¼Îµ ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÏƒÏ„Î®Î»Î· 'DUTY_ID' ÎºÎ±Î¹ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Î¿.
    """
    df = df.copy()
    unique_duties = df['DUTY_COMPANYCODE'].dropna().unique()
    duty_map = {

        duty: get_unique_9digit(used_duty_ids)
        for duty in unique_duties if '_' in duty
    }
    df['DUTY_ID'] = df['DUTY_COMPANYCODE'].map(duty_map)
    df.sort_values(['DUTY_COMPANYCODE', 'START_TIME'], inplace=True)
    return df
def make_short_duty(df):
    """
    Î Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î¿ â€œdoneâ€ DataFrame (Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Î®Î´Î· ÏƒÏ„Î®Î»Î· 'DUTY_ID'),
    Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï€ÎµÎ´Î¯Î¿ â€œShortDutyâ€ â€“ Î¼Î¯Î± Î³ÏÎ±Î¼Î¼Î® Î±Î½Î¬ DUTY_COMPANYCODE
    Î¼Îµ ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿ START_TIME & Î¼Î­Î³Î¹ÏƒÏ„Î¿ END_TIME.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ summary DataFrame ÏƒÏ„Î¹Ï‚ Î¯Î´Î¹ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚.
    """
    print(f"â¡ï¸ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ShortDuty Î±Ï€ÏŒ: ")
    original_columns = df.columns.tolist()

    grouped = df.groupby('DUTY_COMPANYCODE')
    summary_rows = []

    for duty, group in grouped:
        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ START_TIME (Ï‰Ï‚ string)
        group_sorted_by_start = group.sort_values(by='START_TIME', ascending=True, na_position='last')
        first_row = group_sorted_by_start.iloc[0]
        first_start_node = first_row['STARTNODE_COMPANYCODE']
        first_start_time = first_row['START_TIME']
        first_start_basin = first_row.get('STARTNODE_BASIN', '')
        first_start_comp = first_row.get('STARTNODE_COMPANY', '')

        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ END_TIME (Ï‰Ï‚ string)
        group_sorted_by_end = group.sort_values(by='END_TIME', ascending=True, na_position='last')
        last_row = group_sorted_by_end.iloc[-1]
        last_end_node = last_row['ENDNODE_COMPANYCODE']
        last_end_time = last_row['END_TIME']
        last_end_basin = last_row.get('ENDNODE_BASIN', '')
        last_end_comp = last_row.get('ENDNODE_COMPANY', '')

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ…Î½Î¿Ï€Ï„Î¹ÎºÎ®Ï‚ Î³ÏÎ±Î¼Î¼Î®Ï‚
        summary = first_row.copy()

        summary['STARTNODE_COMPANYCODE'] = first_start_node
        summary['STARTNODE_BASIN'] = first_start_basin
        summary['STARTNODE_COMPANY'] = first_start_comp
        summary['START_TIME'] = first_start_time

        summary['ENDNODE_COMPANYCODE'] = last_end_node
        summary['ENDNODE_BASIN'] = last_end_basin
        summary['ENDNODE_COMPANY'] = last_end_comp
        summary['END_TIME'] = last_end_time

        summary_rows.append(summary)

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± DataFrame
    short_df = pd.DataFrame(summary_rows)

    # ÏƒÏ„Î®Î»ÎµÏ‚ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± ÏƒÎµÎ¹ÏÎ¬
    short_df = short_df[original_columns]
    return short_df


# ====================================TIMETABLE=================================
##Î´Î¹Î±Î³ÏÎ±Ï†Ï‰ Î±Î½ Ï…Ï€Î±ÏÏ‡ÎµÎ¹ Ï„Î¿Î½ Ï†Î±ÎºÎµÎ»Î¿ timetable-output
def clean_timetable_dir():
    if os.path.exists(timetable_dir):
        shutil.rmtree(timetable_dir)
    os.makedirs(timetable_dir,exist_ok=True)




def timetable_and_combine():
    print("â–¶ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Timetable...")
#def timetable(input_dir,filename,sheet_name):
    # ----------- Î£Ï„Î±Ï„Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± -------------
    patterns_df = pd.read_csv(os.path.join(static_dir, 'Pattern.csv'), sep=';', dtype=str)
    length_df = pd.read_csv(os.path.join(static_dir, 'PatternAttributes.csv'), sep=';', dtype=str)
    patterns_df.rename(columns={'PATTERNCODE': 'PATTERN'}, inplace=True)
    patterns_df['PATTERN'] = patterns_df['PATTERN'].str.strip()
    patterns_df['DIRECTION'] = patterns_df['DIRECTION'].str.strip()
    length_df['PATTERNCODE'] = length_df['PATTERNCODE'].str.strip()
    length_df['CONVLENGTH'] = length_df['CONVLENGTH'].str.strip()
    operating_days_df = pd.read_csv(os.path.join(static_dir, 'Operating_Days.csv'), sep=';', dtype=str)
    valid_operating_days = set(operating_days_df['Operating_Days'].dropna().str.strip())
    invalid_log_lines = []
    # ----------- Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎºÎ¬Î¸Îµ input Î±ÏÏ‡ÎµÎ¯Î¿Ï… -------------
    for filename in os.listdir(input_dir):
        if not filename.endswith('.xlsx'):
            continue

        input_path = os.path.join(input_dir, filename)
        base_name = os.path.splitext(filename)[0]
        xls = pd.ExcelFile(input_path)



        for sheet_name in xls.sheet_names:
            print(f" Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {filename} | Î¦ÏÎ»Î»Î¿: {sheet_name}")
            print("ğŸ“Œ Î£Ï„Î®Î»ÎµÏ‚ ÏƒÏ„Î¿ Pattern.csv:", patterns_df.columns.tolist())

            # Î”Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï‰Ï‚ string ÏÏƒÏ„Îµ Î½Î± Ï€Î¹Î¬ÏƒÎ¿Ï…Î¼Îµ Î¼Î¿ÏÏ†Î­Ï‚ "0:00", "23:50", "1/1/1900 00:03" Îº.Î»Ï€.
            input_df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)
            # LOG
            # ÎœÎ­Ï„ÏÎ·ÏƒÎ· "1900-01-01" Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿
            count_ana = input_df['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'].astype(str).str.contains('1900-01-01', na=False).sum()
            count_term = input_df['Î¤Î•Î¡ÎœÎ‘'].astype(str).str.contains('1900-01-01', na=False).sum()
#DEEBUG#
            #print(f" Î‘Î½Î¹Ï‡Î½ÎµÏÎ¸Î·ÎºÎ±Î½ {count_ana} Ï†Î¿ÏÎ­Ï‚ '1900-01-01' ÏƒÏ„Î·Î½ Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—")
            #print(f" Î‘Î½Î¹Ï‡Î½ÎµÏÎ¸Î·ÎºÎ±Î½ {count_term} Ï†Î¿ÏÎ­Ï‚ '1900-01-01' ÏƒÏ„Î¿ Î¤Î•Î¡ÎœÎ‘")
            count_ana1 = input_df['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'].astype(str).str.contains('1900-01-02', na=False).sum()
            count_term1 = input_df['Î¤Î•Î¡ÎœÎ‘'].astype(str).str.contains('1900-01-02', na=False).sum()
            #print(f" Î‘Î½Î¹Ï‡Î½ÎµÏÎ¸Î·ÎºÎ±Î½ {count_ana1} Ï†Î¿ÏÎ­Ï‚ '1900-01-02' ÏƒÏ„Î·Î½ Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—")
            #print(f" Î‘Î½Î¹Ï‡Î½ÎµÏÎ¸Î·ÎºÎ±Î½ {count_term1} Ï†Î¿ÏÎ­Ï‚ '1900-01-02' ÏƒÏ„Î¿ Î¤Î•Î¡ÎœÎ‘")

            #  Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï„Î¿Ï… helper ÏƒÎµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚
            input_df['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'] = input_df['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—'].apply(fix_excel_datetime_to_extended_hour_format)
            input_df['Î¤Î•Î¡ÎœÎ‘'] = input_df['Î¤Î•Î¡ÎœÎ‘'].apply(fix_excel_datetime_to_extended_hour_format)

            #  Î”Î¹Î¿ÏÎ¸ÏÏƒÎµÎ¹Ï‚ overlapping
            input_df = adjust_overlapping_times(input_df)

            input_df['PATTERN'] = input_df['PATTERN'].str.strip()
            input_df['MERGE_KEY'] = input_df.index
            input_df['LINE_EXCEL'] = input_df['LINE']
            input_df['EXCEL_NAME'] = filename
            #print (input_df)


    # LINE_EXCEL
            #output_df = output_df.merge(input_df[['MERGE_KEY', 'LINE_EXCEL']], on='MERGE_KEY', how='left')
# DEEBUG#
            # print(input_df[['Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—','Î¤Î•Î¡ÎœÎ‘']].head())
            # print("ğŸ“Œ Î£Ï„Î®Î»ÎµÏ‚ ÏƒÏ„Î¿ Pattern.csv:", patterns_df.columns.tolist())

            #  Merge Î¼Îµ patterns_df
            #  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± unique map PATTERN â†’ DIRECTION
            # Î±Ï†Î±Î¹ÏÏ direction  Î±Ï€Î¿ input  Î³Î¹Î± Î½Î± Î¼Î·Î½ Î¼ÎµÎ¹Î½ÎµÎ¹ Î· Ï€Î±Î»Î¹Î± ÏƒÏ„Î·Î»Î·
            input_df = input_df.drop(columns=['DIRECTION'], errors='ignore')
            input_df["LINE"]=None
            #input_df = input_df.drop(columns=['LINE'], errors='ignore')
            #input_df = input_df.rename(columns={'LINE': 'LINE'}, inplace=True)
            # input_df = input_df.join(pattern_dir_map, on='PATTERN')
            merged = input_df.merge(
                patterns_df[['PATTERN', 'NODE', 'DIRECTION','LINE']],
                on='PATTERN', how='left'
            ).dropna(subset=['NODE'])
# DEEBUG#
            #print(merged[['PATTERN', 'NODE']].drop_duplicates().head(10))

            #  Merge Î¼Îµ length_df
            merged = merged.merge(
                length_df[['PATTERNCODE', 'CONVLENGTH']].rename(columns={
                    'PATTERNCODE': 'PATTERN',
                    'CONVLENGTH': 'LENGTH'
                }),
                on='PATTERN', how='left'
            )

            #print(merged.head())
            merged.drop(columns=['LINE_x'], errors='ignore')
            merged.rename(columns={'LINE_y': 'LINE'}, inplace=True)

            #print(merged.head())
            #  Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ„Î¹Ï‚ 3 Ï€ÏÏÏ„ÎµÏ‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î±Î½Î¬ Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î¹Î¿
            try:
                merged = merged.groupby('MERGE_KEY').head(3)
            except Exception as e:
                print(e)
##DEEBUG#
            #print('merged arxika')
            #print(merged[['PATTERN', 'NODE', 'LENGTH']].head())
            #  ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„ÎµÎ»Î¹ÎºÏÎ½ ÏƒÏ„Î·Î»ÏÎ½
            target_columns = [
                'SERVICELEVEL', 'CALENDAR', 'OPERATINGDAY', 'PIECETYPE', 'STARTINGDATE',
                'ENDINGDATE', 'LINE', 'LINEBASIN', 'LINECOMPANY', 'PATTERN', 'DIRECTION',
                'TRIPCODE', 'TRIPCOMPANYCODE', 'ACTIVITYNUMBER', 'LENGTH', 'VEHBLOCKID',
                'DEPOT', 'VEHICLETYPE', 'VBLINE', 'VB_LINEBASIN', 'VB_LINECOMPANY',
                'PIECEORDER', 'ARRIVALTIME', 'DEPARTURETIME', 'NODE', 'NODEBASIN',
                'NODECOMPANY', 'PASSAGEORDER', 'BLOCK','LINE_EXCEL', 'EXCEL_NAME'
            ]
            output_df = pd.DataFrame(columns=target_columns)

            #  Mapping Î±Ï€ÏŒ merged -> output_df
            column_mapping = {
                'LINE': 'LINE',
                'LINE_EXCEL': 'LINE_EXCEL',
                'PATTERN': 'PATTERN',
                'DIRECTION': 'DIRECTION',
                'NODE': 'NODE',
                'LENGTH': 'LENGTH',
                'LINEBASIN': 'LINEBASIN',
                'OPERATINGDAY': 'OPERATINGDAY',
                'STARTINGDATE': 'STARTINGDATE',
                'ENDINGDATE': 'ENDINGDATE',
                'CALENDAR': 'CALENDAR',
                'MERGE_KEY': 'MERGE_KEY',
                'VB_COMPANYCODE': 'BLOCK',
                'EXCEL_NAME':'EXCEL_NAME'
            }

            for src, dst in column_mapping.items():
                if src in merged.columns:
                    output_df[dst] = merged[src]


            #output_df['LINE_EXCEL'] = input_df['LINE_EXCEL']

            #  Î£Ï„Î±Î¸ÎµÏÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
            output_df['SERVICELEVEL'] = 'F1'
            output_df['PIECETYPE'] = 'Revenue'
            output_df['LINECOMPANY'] = 'Def'
            output_df['NODEBASIN'] = 'OSETH'
            output_df['NODECOMPANY'] = 'Def'
            #  ÎœÎ¿Î½Î±Î´Î¹ÎºÎ¬ IDs & PASSAGEORDER
            unique_ids_per_group = {

                key: get_unique_9digit(used_trip_ids)
                for key in output_df['MERGE_KEY'].unique()
            }
            output_df['TRIPCODE'] = output_df['MERGE_KEY'].map(unique_ids_per_group)
            output_df['TRIPCOMPANYCODE'] = output_df['TRIPCODE']
            output_df['ACTIVITYNUMBER'] = output_df['TRIPCODE']
            output_df['PASSAGEORDER'] = output_df.groupby('MERGE_KEY').cumcount() + 1

            ############isos to vgalo gia debugging

            #  assign_times Î¼Îµ debug print Î³Î¹Î± Î½Î± ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ‰ "00:02:00"
            def assign_times(group):
                try:
                    dep_time = input_df.loc[group['MERGE_KEY'].iloc[0], 'Î‘ÎÎ‘Î§Î©Î¡Î—Î£Î—']
                    arr_time = input_df.loc[group['MERGE_KEY'].iloc[0], 'Î¤Î•Î¡ÎœÎ‘']
                    count = group.shape[0]
#DEBUGGING ORES
                    # Î•ÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Î³Î¹Î± debugging:
                    #print(f"DEBUG MERGE_KEY={group['MERGE_KEY'].iloc[0]}:")
                    #print(f"  dep_time (extended) = {dep_time}")
                    #print(f"  arr_time (extended) = {arr_time}")

                    for idx, row in group.iterrows():
                        po = row['PASSAGEORDER']
                        if po == 1:
                            group.at[idx, 'ARRIVALTIME'] = dep_time
                            group.at[idx, 'DEPARTURETIME'] = dep_time

                        elif po == 2 and count == 2:
                            group.at[idx, 'ARRIVALTIME'] = arr_time
                            group.at[idx, 'DEPARTURETIME'] = arr_time

                        elif po == 2 and count == 3:
                            mid = average_time(dep_time, arr_time)
#DEEBUG#
                           # print(f"  midpoint = average_time({dep_time}, {arr_time}) â†’ {mid}")  # DEBUG
                            group.at[idx, 'ARRIVALTIME'] = mid
                            group.at[idx, 'DEPARTURETIME'] = mid

                        elif po == 3:
                            group.at[idx, 'ARRIVALTIME'] = arr_time
                            group.at[idx, 'DEPARTURETIME'] = arr_time

                except Exception as e:
                    print(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ ÏÏÎ± Î³Î¹Î± MERGE_KEY={group['MERGE_KEY'].iloc[0]}: {e}")
                return group

            output_df = (
                output_df.groupby('MERGE_KEY', group_keys=False, sort=False).apply(assign_times).reset_index(drop=True)
                )

            #  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î·Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ DUTY
            def create_duty(row):
                duty_code = row['DUTY_COMPANYCODE']
                if pd.isna(duty_code):
                    return ''
                elif '_' in duty_code:
                    return duty_code
                else:
                    return f"{row['VB_COMPANYCODE']}_{duty_code}"

            duty_mapping = input_df[['MERGE_KEY', 'VB_COMPANYCODE', 'DUTY_COMPANYCODE']].copy()
            duty_mapping['DUTY'] = duty_mapping.apply(create_duty, axis=1)
            output_df = output_df.merge(
                duty_mapping[['MERGE_KEY', 'DUTY']], on='MERGE_KEY', how='left'
            )

            #  ÎœÎ¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¹ÏÎ½
            output_df['STARTINGDATE'] = pd.to_datetime(
                output_df['STARTINGDATE'], errors='coerce'
            ).dt.strftime('%Y-%m-%d')
            output_df['ENDINGDATE'] = pd.to_datetime(
                output_df['ENDINGDATE'], errors='coerce'
            ).dt.strftime('%Y-%m-%d')



            output_df.drop(columns=['MERGE_KEY'], inplace=True)


            ############### Î•Î¾Î±Î³Ï‰Î³Î® Timetable CSV
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î¼Î· Î­Î³ÎºÏ…ÏÎ± OPERATINGDAY values
            invalid_days = set(output_df['OPERATINGDAY'].dropna().unique()) - valid_operating_days
            if invalid_days:
                message=print(f"âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ OPERATINGDAY ÏƒÏ„Î¿ {base_name} - {sheet_name}: {sorted(invalid_days)}")
                #print(message)
                invalid_log_lines.append(message)

            safe_sheet = sheet_name.replace(" ", "_").replace("/", "-")


            #Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹Î± Ï†Î±ÎºÎ­Î»Ï‰Î½ Î³Î¹Î± ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î® ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ outputs
            line_folder = os.path.join(timetable_dir, base_name)
            os.makedirs(line_folder, exist_ok=True)
            output_path = os.path.join(line_folder, f"{base_name}_Timetable_{safe_sheet}.csv")
            output_df.to_csv(output_path, index=False, sep=';', encoding='utf-8')
            print(f"âœ… Timetable Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {output_path}")

#LINEBASIN ÏƒÏ„Î¿ ÏŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…
            # # Î‘Ï€ÏŒÎºÏ„Î·ÏƒÎ· Ï„Î¹Î¼Î®Ï‚ LINEBASIN (Ï€.Ï‡. 'Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·') ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚
            # linebasin_values = output_df['LINEBASIN'].dropna().unique()
            # linebasin_str = str(linebasin_values[0]).replace(" ", "_") if len(linebasin_values) > 0 else "UnknownBasin"
            # # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï†Î±ÎºÎ­Î»Î¿Ï… Î³Î¹Î± ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î®
            # line_folder = os.path.join(timetable_dir, base_name)
            # os.makedirs(line_folder, exist_ok=True)
            # # Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Ï„Î¿Ï… LINEBASIN ÏƒÏ„Î¿ ÏŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…
            # output_filename = f"{base_name}_{linebasin_str}_Timetable_{safe_sheet}.csv"
            # output_path = os.path.join(line_folder, output_filename)

            # Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï†Î±ÎºÎ­Î»Î¿Ï… meres ÎºÎ±Î¹ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ txt Î³Î¹Î± ÎºÎ¬Î¸Îµ sheet_name/Î¼ÎµÏÎ±
            meres_folder = os.path.join(timetable_dir, 'meres')
            os.makedirs(meres_folder, exist_ok=True)
            meres_filename = f"{base_name}_meres.txt"
            meres_path = os.path.join(meres_folder, meres_filename)
            if not os.path.exists(meres_path):
                with open(meres_path, 'w', encoding='utf-8') as f:
                    f.write("Sheet_name;Meres\n")
            with open(meres_path, 'a', encoding='utf-8') as f:
                f.write(f"{sheet_name};\n")


            invalid_days = set(output_df['OPERATINGDAY'].dropna().unique()) - valid_operating_days
            if invalid_days:
                msg = f"âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ OPERATINGDAY ÏƒÏ„Î¿ {base_name} - {sheet_name}: {sorted(invalid_days)}"
                print(msg)
                invalid_log_lines.append(msg)
            write_invalid_days_log(invalid_log_lines, timetable_dir)

#Î‘Î¦ÎŸÎ¥ Î•Î§Î© Î¦Î¤Î™Î‘ÎÎ•Î™ ÎŸÎ›Î‘ Î¤Î‘ Î‘Î¡Î§Î•Î™Î‘ Î¤Î™ÎœÎ•Î¤Î‘Î’Î›Î•, Î¤Î‘ Î•ÎÎ©ÎÎ© Î£Î• Î•ÎÎ‘
    all_dfs = []

    # Î‘Î½Î±Î´ÏÎ¿Î¼Î¹ÎºÎ® Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ .csv
    for root, _, files in os.walk(timetable_dir):
        if 'meres' in root.lower():
            continue  # Î±Î³Î½ÏŒÎ·ÏƒÎµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ meres
        for file in files:
            if file.endswith('.csv') and '_Timetable_' in file:
                file_path = os.path.join(root, file)
                try:
                    df = pd.read_csv(file_path, sep=';', dtype=str)
                    df['SOURCE_FILE'] = file  # optional: ÎºÏÎ±Ï„Î¬ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Ï€ÏÎ¿Î­Î»ÎµÏ…ÏƒÎ·Ï‚
                    all_dfs.append(df)
                except Exception as e:
                    print(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… {file_path}: {e}")

    if all_dfs:
        combined_df = pd.concat(all_dfs, ignore_index=True)
        combined_output_path = 'output/combined_all_timetables.csv'
        os.makedirs(os.path.dirname(combined_output_path), exist_ok=True)
        combined_df.to_csv(combined_output_path, sep=';', index=False, encoding='utf-8')
        print(f"âœ… Î£Ï…Î½ÎµÎ½ÏÎ¸Î·ÎºÎ±Î½ {len(all_dfs)} Î±ÏÏ‡ÎµÎ¯Î± timetable â†’ {combined_output_path}")
    else:
        print("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±ÏÏ‡ÎµÎ¯Î± timetable Î³Î¹Î± ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ·.")





# ====================================Blocks=================================
##Î´Î¹Î±Î³ÏÎ±Ï†Ï‰ Î±Î½ Ï…Ï€Î±ÏÏ‡ÎµÎ¹ Ï„Î¿Î½ Ï†Î±ÎºÎµÎ»Î¿ timetable-output
def clean_blocks_dir():
    if os.path.exists(blocks_dir):
        shutil.rmtree(blocks_dir)
    os.makedirs(blocks_dir,exist_ok=True)

# def blocks():
#     print("â–¶ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Blocks...")
#     # read days_mapping and for each correct_depot , for each operacting ( of selected correct depot) create a file for block ( same logic as before) ->  (correct_depot + OPERATINGDAY + DayType).csv
#     # at the end we merge all csv into one for same correct_depot and Daytype
#
#     combined_output_path = 'output/combined_all_timetables.csv'
#     mapping_path = 'output/expanded_days_mapping.csv'
#     blocks_dir="blocks-output"
#     timetable_df = pd.read_csv(combined_output_path, sep=';', dtype=str)
#     mapping_df = pd.read_csv(mapping_path, sep=';', dtype=str)
#     os.makedirs(blocks_dir, exist_ok=True)
#
#
#     generated_files = []
#
#     # Ï†Î¹Î»Ï„ÏÎ±ÏÎ¹ÏƒÎ¼Î±
#     for _, row in mapping_df.iterrows():
#         depot = row['correct_depot']
#         op_day = row['OPERATINGDAY']
#         day_type = row['DayType']
#
#         """
#         subset = timetable_df[
#             (timetable_df['correct_depot'] == depot) &
#             (timetable_df['OPERATINGDAY'] == op_day)
#             ]
#         print(f"[DEBUG] {depot=} {op_day=} {day_type=} subset rows={len(subset)}")
#
#         if subset.empty:
#             print(f"âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î³Î¹Î± {depot}, {op_day}, {day_type}")
#             continue
#         """
#         if ((timetable_df['correct_depot'] == depot) & (timetable_df['OPERATINGDAY'] == op_day)).any():
#
#             subset = timetable_df[
#                 (timetable_df['correct_depot'] == depot) &
#                 (timetable_df['OPERATINGDAY'] == op_day)
#                 ]
#             if not subset.empty:
#                 print ("is empty")
#
#             #print(subset.head())
#
#             #read days_mapping and for each correct_depot , for each operacting ( of selected correct depot) create a file for block ( same logic as before) ->  (correct_depot + OPERATINGDAY + DayType).csv
#             #at the end we merge all csv into one for same correct_depot and Daytype
#
#             # if not file.endswith('.csv'):
#             #     continue
#             # output_df = pd.read_csv(os.path.join(timetable_dir, file), sep=';', dtype=str)
#
#             #rint (subset['PASSAGEORDER'] == '1')
#
#
#             first = subset[subset['PASSAGEORDER'] == '1'].copy()
#             #print(first.head())
#             last = timetable_df[
#                 timetable_df.groupby('TRIPCODE')['PASSAGEORDER'].transform('max') == timetable_df['PASSAGEORDER']
#             ].copy()
#             #print(last.head())
#             print(f"[DEBUG] found {len(first)} Ï€ÏÏÏ„ÎµÏ‚ ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ {len(last)} Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚")
#
#             if first.empty or last.empty:
#                 print(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ·: Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î­Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î³Î¹Î± {depot}, {op_day}, {day_type}")
#                 continue
#             merged2 = first.merge(
#                 last[['TRIPCODE','NODE','ARRIVALTIME']],
#                 on='TRIPCODE',
#                 suffixes=('_START','_END')
#             )
#             #print(merged2.head())
#
#             output2 = pd.DataFrame({
#                 'PROJUNIT_NAME':  day_type,
#                 'PROJECT_NAME':     depot,
#                 'VB_COMPANYCODE':   merged2['BLOCK'],
#                 'VEHBLOCK_ID':      merged2['BLOCK'],
#                 'PIECETYPE_NAME':   'Revenue',
#                 'LINE_COMPANYCODE': merged2['LINE'],
#                 'LINE_BASIN':       merged2['LINEBASIN'],
#                 'LINE_COMPANY':     'Def',
#                 'JOURNEY_ID':       merged2['TRIPCODE'],
#                 'TRIP_ID':          merged2['TRIPCODE'],
#                 'STARTNODE_COMPANYCODE': merged2['NODE_START'],
#                 'STARTNODE_BASIN':       'OSETH',
#                 'STARTNODE_COMPANY':     'Def',
#                 'ENDNODE_COMPANYCODE':   merged2['NODE_END'],
#                 'ENDNODE_BASIN':         'OSETH',
#                 'ENDNODE_COMPANY':       'Def',
#                 'START_TIME':            merged2['DEPARTURETIME'],
#                 'END_TIME':              merged2['ARRIVALTIME_END'],
#                 'RECHARGETYPE_NAME':     [None] * len(merged2),
#                 'DUTY':                  merged2['DUTY']
#             })
#             output2['VEHBLOCK_ID'] = output2['VEHBLOCK_ID'].apply(replace_if_contains_letter)
#
#             output2.sort_values(['VB_COMPANYCODE','START_TIME'], inplace=True)
#             #print(output2.head())
#
#             filename = f"{depot}_{op_day}_{day_type}_blocks.csv".replace(" ", "_")
#             filepath = os.path.join(blocks_dir, filename)
#             output2.to_csv(filepath, sep=';', index=False, encoding='utf-8')
#             generated_files.append(filename)
#             print(f"âœ… Blocks Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿ {filename}")

def blocks_and_mergedDepotDaytype():
    print("â–¶ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Blocks...")
    # read days_mapping and for each correct_depot , for each operacting ( of selected correct depot) create a file for block ( same logic as before) ->  (correct_depot + OPERATINGDAY + DayType).csv
    # at the end we merge all csv into one for same correct_depot and Daytype

    combined_output_path = 'output/combined_all_timetables.csv'
    mapping_path = 'output/expanded_days_mapping.csv'
    blocks_dir = "blocks-output"
    timetable_df = pd.read_csv(combined_output_path, sep=';', dtype=str)
    mapping_df = pd.read_csv(mapping_path, sep=';', dtype=str)
    os.makedirs(blocks_dir, exist_ok=True)
    combined_dir = os.path.join(blocks_dir, 'combined')
    os.makedirs(combined_dir, exist_ok=True)

    # generated_files = []
    generated_files: list[tuple[str, str, str]] = []

    # Ï†Î¹Î»Ï„ÏÎ±ÏÎ¹ÏƒÎ¼Î±
    for _, row in mapping_df.iterrows():
        depot = row['correct_depot']
        op_day = row['OPERATINGDAY']
        day_type = row['DayType']

        """
        subset = timetable_df[
            (timetable_df['correct_depot'] == depot) &
            (timetable_df['OPERATINGDAY'] == op_day)
            ]
        print(f"[DEBUG] {depot=} {op_day=} {day_type=} subset rows={len(subset)}")

        if subset.empty:
            print(f"âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î³Î¹Î± {depot}, {op_day}, {day_type}")
            continue
        """
        if ((timetable_df['correct_depot'] == depot) & (timetable_df['OPERATINGDAY'] == op_day)).any():

            subset = timetable_df[
                (timetable_df['correct_depot'] == depot) &
                (timetable_df['OPERATINGDAY'] == op_day)
                ]
            if not subset.empty:
                print("is empty")

            # print(subset.head())

            # read days_mapping and for each correct_depot , for each operacting ( of selected correct depot) create a file for block ( same logic as before) ->  (correct_depot + OPERATINGDAY + DayType).csv
            # at the end we merge all csv into one for same correct_depot and Daytype

            # if not file.endswith('.csv'):
            #     continue
            # output_df = pd.read_csv(os.path.join(timetable_dir, file), sep=';', dtype=str)

            # rint (subset['PASSAGEORDER'] == '1')

            first = subset[subset['PASSAGEORDER'] == '1'].copy()
            # print(first.head())
            last = timetable_df[
                timetable_df.groupby('TRIPCODE')['PASSAGEORDER'].transform('max') == timetable_df['PASSAGEORDER']
                ].copy()
            # print(last.head())
            print(f"[DEBUG] found {len(first)} Ï€ÏÏÏ„ÎµÏ‚ ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ {len(last)} Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚")

            if first.empty or last.empty:
                print(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ·: Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î­Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î³Î¹Î± {depot}, {op_day}, {day_type}")
                continue
            merged2 = first.merge(
                last[['TRIPCODE', 'NODE', 'ARRIVALTIME']],
                on='TRIPCODE',
                suffixes=('_START', '_END')
            )
            # print(merged2.head())

            output2 = pd.DataFrame({
                'PROJUNIT_NAME': day_type,
                'PROJECT_NAME': depot,
                'VB_COMPANYCODE': merged2['BLOCK'],
                'VEHBLOCK_ID': merged2['BLOCK'],
                'PIECETYPE_NAME': 'Revenue',
                'LINE_COMPANYCODE': merged2['LINE'],
                'LINE_BASIN': merged2['LINEBASIN'],
                'LINE_COMPANY': 'Def',
                'JOURNEY_ID': merged2['TRIPCODE'],
                'TRIP_ID': merged2['TRIPCODE'],
                'STARTNODE_COMPANYCODE': merged2['NODE_START'],
                'STARTNODE_BASIN': 'OSETH',
                'STARTNODE_COMPANY': 'Def',
                'ENDNODE_COMPANYCODE': merged2['NODE_END'],
                'ENDNODE_BASIN': 'OSETH',
                'ENDNODE_COMPANY': 'Def',
                'START_TIME': merged2['DEPARTURETIME'],
                'END_TIME': merged2['ARRIVALTIME_END'],
                'RECHARGETYPE_NAME': [None] * len(merged2),
                'DUTY': merged2['DUTY']
            })
            output2['VEHBLOCK_ID'] = output2['VEHBLOCK_ID'].apply(replace_if_contains_letter)

            output2.sort_values(['VB_COMPANYCODE', 'START_TIME'], inplace=True)
            # print(output2.head())

            filename = f"{depot}_{op_day}_{day_type}_blocks.csv".replace(" ", "_")
            filepath = os.path.join(blocks_dir, filename)
            output2.to_csv(filepath, sep=';', index=False, encoding='utf-8')
            # generated_files.append(filename)
            generated_files.append((depot, day_type, filepath))

            print(f"âœ… Blocks Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿ {filename}")

    """
        Î“Î¹Î± ÎºÎ¬Î¸Îµ DayType Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î± generated block Î±ÏÏ‡ÎµÎ¯Î±,
        Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ ÏŒÎ»Î± Ï„Î± blocks-*.csv Î¼Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ DayType ÎºÎ±Î¹ Ï„Î± ÎµÎ½ÏÎ½ÎµÎ¹
        ÏƒÎµ Î­Î½Î± combined.csv.
        """

    # files = [f for f in os.listdir(blocks_dir) if f.endswith('_blocks.csv')]
    # groups = defaultdict(list)
    groups: dict[tuple[str, str], list[str]] = defaultdict(list)

    for depot, day_type, path in generated_files:
        groups[(depot, day_type)].append(path)

    # Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¿Î¼Î¬Î´Î±, Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ & concat
    for (depot, day_type), paths in groups.items():
        dfs = [pd.read_csv(p, sep=';', dtype=str) for p in paths]
        merged = pd.concat(dfs, ignore_index=True)

        out_name = f"{depot}_{day_type}_combined.csv".replace(" ", "_")
        out_path = os.path.join(combined_dir, out_name)
        merged.to_csv(out_path, sep=';', index=False, encoding='utf-8')
        print(f"âœ… Merged {len(paths)} â†’ {out_name}")




###################### Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Duties CSV

        # ====================================Blocks=================================

def clean_duties_dir():
    if os.path.exists(duties_dir):
        # Î ÏÎ¹Î½ ÎºÎ¬Î½ÎµÎ¹Ï‚ cleanup ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Î¼Îµ shutil
        for handler in logging.root.handlers[:]:
            handler.close()
            logging.root.removeHandler(handler)
        shutil.rmtree(duties_dir)
    os.makedirs(duties_dir,exist_ok=True)

def setup_log():
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï†Î±ÎºÎ­Î»Î¿Ï… log Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹
    log_dir = 'duties-output/final'
    os.makedirs(log_dir, exist_ok=True)
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼Î¿Î½Î±Î´Î¹ÎºÎ¿Ï Î¿Î½ÏŒÎ¼Î±Ï„Î¿Ï‚ log Î¼Îµ timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(log_dir, f'log_final_duties_{timestamp}.txt')
    # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï€Î±Î»Î±Î¹ÏÎ½ handlers (Î±Î½ Î­Ï‡ÎµÎ¹Ï‚ Î¾Î±Î½Î±Ï„ÏÎ­Î¾ÎµÎ¹ logging.basicConfig)
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    # Î¡ÏÎ¸Î¼Î¹ÏƒÎ· logging Î¼ÏŒÎ½Î¿ ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿
    logging.basicConfig(
        filename=log_path,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8'
    )


def duties():
    if not os.path.exists("blocks-output/combined"):
        os.mkdir("blocks-output/combined")
    combined_blocks_dir = "blocks-output/combined"
    duties_dir = "duties-output"
    os.makedirs(duties_dir, exist_ok=True)
    short_dir = os.path.join(duties_dir, 'short')
    os.makedirs(short_dir, exist_ok=True)
    print("â–¶ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Dutiesâ€¦")

    for file in os.listdir(combined_blocks_dir):
        if not file.endswith('_combined.csv'):
            continue
        # output3 = pd.read_csv(os.path.join(blocks_dir, file), sep=';', dtype=str)

        base = file[:-len("_combined.csv")]
        blocks_fp = os.path.join(combined_blocks_dir, file)
        df_blocks = pd.read_csv(blocks_fp, sep=';', dtype=str)

        df_duties = pd.DataFrame({
            'VERSION': "VERSION_U",
            'PROJECT_NAME': df_blocks['PROJECT_NAME'],
            'PROJUNIT_NAME': df_blocks['PROJUNIT_NAME'],
            'DUTY_COMPANYCODE': df_blocks['DUTY'],
            'DUTY_ID': df_blocks['JOURNEY_ID'],
            'PIECETYPE_NAME': 'VB_Piece',
            'REFERREDVB_COMPANYCODE': df_blocks['VB_COMPANYCODE'],
            'PRETIMESEC': [None] * len(df_blocks),
            'POSTTIMESEC': [None] * len(df_blocks),
            'STARTNODE_COMPANYCODE': df_blocks['STARTNODE_COMPANYCODE'],
            'STARTNODE_BASIN': 'OSETH',
            'STARTNODE_COMPANY': 'Def',
            'ENDNODE_COMPANYCODE': df_blocks['ENDNODE_COMPANYCODE'],
            'ENDNODE_BASIN': 'OSETH',
            'ENDNODE_COMPANY': 'Def',
            'START_TIME': df_blocks['START_TIME'],
            'END_TIME': df_blocks['END_TIME']
        })

        df_duties.sort_values(['DUTY_COMPANYCODE', 'START_TIME'], inplace=True)
        duties_filename = f"{base}_Duties_.csv"
        duties_path = os.path.join(duties_dir, duties_filename)
        df_duties.to_csv(duties_path, index=False, sep=';', encoding='utf-8')
        print(f"   â€¢ Saved {duties_filename}")

        # ----- Î•Ï†Î±ÏÎ¼Î¿Î³Î® shortDuty logic (ÎµÎ½ÏƒÏ‰Î¼Î±Ï„Ï‰Î¼Î­Î½Î¿) -----

        #  Î‘Î½Î¬Î¸ÎµÏƒÎ· DUTY_ID
        df_done = pd.read_csv(duties_path, sep=';', dtype=str)
        df_done = assign_duty_ids(df_done)
        # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î· short
        df_short = make_short_duty(df_done)

        #  Î•Î¾Î±Î³Ï‰Î³Î® Î½Î­Î¿Ï… CSV Î¼Îµ Ï€ÏÏŒÎ¸ÎµÎ¼Î± 'short_'
        # done_name = f"done{duties_filename}"
        # done_path = os.path.join(duties_dir, done_name)
        # df_done.to_csv(done_path, index=False, sep=';', encoding='utf-8')
        # print(f"   Done Duties (Î¼Îµ DUTY_ID) saved to: {done_path}")

        short_name = f"short_{duties_filename}"
        short_path = os.path.join(short_dir, short_name)
        df_short.to_csv(short_path, index=False, sep=';', encoding='utf-8')
        print(f"  Short Duties (Î¼Îµ DUTY_ID) saved to: {short_path}")

    print(f" âœ” ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ:")



def final_duties():
    short_dir = 'duties-output/short'
    final_dir = 'duties-output/final'
    os.makedirs(final_dir, exist_ok=True)
    deadhead_path = 'static/Vehicle_block_deadheads.csv'
    deadhead_df = pd.read_csv(deadhead_path, sep=';', dtype=str)
    mapping_path = 'static/mappings.txt'
    mapping_df = pd.read_csv(mapping_path, sep=';', dtype=str)
    mapping_dict = dict(zip(mapping_df['depot'], mapping_df['mapping']))



    for file in os.listdir(short_dir):
        if not file.endswith('.csv'):
            continue

        short_df = pd.read_csv(os.path.join(short_dir, file), sep=';', dtype=str)
        final_rows = []



        group_keys = ['REFERREDVB_COMPANYCODE', 'PROJECT_NAME', 'PROJUNIT_NAME']
        for (vb_code, project_name, projunit_name), group in short_df.groupby(group_keys):
            group = group.copy()
            if len(group) != 2:
                logging.warning(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· {vb_code} ÏƒÏ„Î¿ {file} (Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ 2 Î³ÏÎ±Î¼Î¼Î­Ï‚)")
                #final_rows.extend(group.to_dict(orient='records'))
                continue

            deadhead_matches = deadhead_df[
                (deadhead_df['VB_COMPANYCODE'] == vb_code) &
                (deadhead_df['PROJECT_NAME'] == project_name) &
                (deadhead_df['PROJUNIT_NAME'] == projunit_name)
                ]
            if deadhead_matches.empty:
                logging.warning(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· {vb_code} ÏƒÏ„Î¿ {file} (Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ deadhead)")
                continue

            first_row = group.iloc[0].copy()
            last_row = group.iloc[1].copy()

            start_time = deadhead_matches['START_TIME'].iloc[0]
            end_time = deadhead_matches['END_TIME'].iloc[-1]

            first_row['START_TIME'] = start_time
            first_row['STARTNODE_COMPANYCODE'] = project_name
            final_rows.append(first_row)

            third_row = last_row.copy()
            third_row['END_TIME'] = end_time
            third_row['ENDNODE_COMPANYCODE'] = project_name

            middle_row = last_row.copy()
            middle_row['PIECETYPE_NAME'] = "Pre_Duty"
            #middle_row['REFERREDVB_COMPANYCODE'] = ''

            depot_node = third_row['STARTNODE_COMPANYCODE']
            middle_row['STARTNODE_COMPANYCODE'] = depot_node
            middle_row['ENDNODE_COMPANYCODE'] = depot_node

            try:
                third_start_time = datetime.strptime(third_row['START_TIME'], "%H:%M:%S")
                middle_row['START_TIME'] = (third_start_time - timedelta(minutes=10)).strftime("%H:%M:%S")
                middle_row['END_TIME'] = third_start_time.strftime("%H:%M:%S")
            except Exception as e:
                logging.warning(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± parsing ÏÏÎ±Ï‚ Î³Î¹Î± {vb_code} ÏƒÏ„Î¿ {file}: {e}")
                continue

            final_rows.extend([middle_row, third_row])

        if final_rows:
            result_df = pd.DataFrame(final_rows)

            #for REFERREDVB_COMPANYCODE_id, group in result_df.groupby('REFERREDVB_COMPANYCODE'):
            for (vb_code, project_name, projunit_name), group in result_df.groupby(group_keys):

                if len(group) == 3:
                    start_node = group.iloc[0]['STARTNODE_COMPANYCODE']
                    end_node = group.iloc[-1]['ENDNODE_COMPANYCODE']


                    if start_node in mapping_dict:
                        result_df.at[group.index[0], 'STARTNODE_BASIN'] = mapping_dict[start_node]
                    else:
                        logging.warning(f"ğŸ” Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ mapping Î³Î¹Î± START: {start_node}")

                    if end_node in mapping_dict:
                        result_df.at[group.index[-1], 'ENDNODE_BASIN'] = mapping_dict[end_node]
                    else:
                        logging.warning(f"ğŸ” Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ mapping Î³Î¹Î± END: {end_node}")


            # Î”Î¹Î±Î³ÏÎ±Ï†Î® REFERREDVB_COMPANYCODE ÏƒÏ„Î· Pre_Duty Î³ÏÎ±Î¼Î¼Î®
            result_df.loc[result_df['PIECETYPE_NAME'] == 'Pre_Duty', 'REFERREDVB_COMPANYCODE'] = ''

            output_path = os.path.join(final_dir, f"final_{file}")
            result_df.to_csv(output_path, sep=';', index=False, encoding='utf-8')
            print(f"âœ… Final Duties Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿: {output_path}")


# ------------------- Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏÎ½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ ------------------------
if final:
    with open(trip_history_file, 'w') as f:
        for uid in sorted(used_trip_ids):
            f.write(uid + '\n')

    with open(duty_history_file, 'w') as f:
        for uid in sorted(used_duty_ids):
            f.write(uid + '\n')

    print("âœ… Î¤Î¿ history/used_*.txt Î­Ï‡ÎµÎ¹ ÎµÎ½Î·Î¼ÎµÏÏ‰Î¸ÎµÎ¯ Î¼Îµ ÏŒÎ»Î± Ï„Î± IDs Ï€Î¿Ï… Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎ±Î½.")
    print(f"Î•Î³Î³ÏÎ±Ï†Î­Ï‚ Ï„Î¿Ï… trip_history ÎµÎ¯Î½Î±Î¹: {len(used_trip_ids)}")
    print(f"Î•Î³Î³ÏÎ±Ï†Î­Ï‚ Ï„Î¿Ï… used_duty_ids ÎµÎ¯Î½Î±Î¹: {len(used_duty_ids)}")
else:
    print("ÎŸÏ‡Î¹ Î±Î»Î»Î±Î³Î­Ï‚ ÏƒÏ„Î± history")




#==========================================meres-diadika apo combined timetable====================================


def create_days_mapping_file(combined_csv_path: str, output_txt_path: str) -> None:
    try:
        df = pd.read_csv(combined_csv_path, sep=';', dtype=str)

        result = df[['correct_depot', 'OPERATINGDAY']].drop_duplicates()


        result = result.sort_values('correct_depot')

        result['DayType'] = None

        result.to_csv(output_txt_path, sep=';', index=False, encoding='utf-8')


        print(f"âœ… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿ mapping: {output_txt_path}")
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± days mapping Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {e}")





def update_days_mapping_from_static(mapping_txt_path, static_csv_path):
    try:
        # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Ï„Ï‰Î½ Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Ï‰Î½

        mapping_df = pd.read_csv(mapping_txt_path, sep=';', dtype=str)
        static_df = pd.read_csv(static_csv_path, sep=';', dtype=str)


        # ÎšÎ±Î¸Î±ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î± ÎºÎµÎ¯Î¼ÎµÎ½Î±
        #mapping_df['DAYS'] = mapping_df['DAYS'].str.strip()
        #static_df['Operating_Days'] = static_df['Operating_Days'].str.strip()
        #static_df['Days_Diadiko'] = static_df['Days_Diadiko'].str.strip()

        # # ÎšÎ¬Î½Î¿Ï…Î¼Îµ merge Î¼Îµ Î²Î¬ÏƒÎ· DAYS <-> Operating_Days
        # merged = mapping_df.merge(
        #     static_df[['Operating_Days', 'Day_Map','Days_Diadiko']],
        #     on ='Operating_Days',
        #     left_index=False,
        #     how='left'
        # )
        # #print(merged.head())

        merged = mapping_df.merge(
            static_df[['Operating_Days', 'Day_Map']],
            right_on='Operating_Days',
            left_on='OPERATINGDAY',
            how='left'
        )
        # Î¤ÎµÎ»Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
        merged = merged[['correct_depot', 'OPERATINGDAY', 'Day_Map']]

        merged["Day_Map"]=None

        # Î‘Î½Ï„Î¹ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ„Î®Î»Î·

        #merged['DAYSDIADIKO'] = merged['Days_Diadiko']
        #final_df = merged[['Day_Map', 'DAYSDIADIKO']]
        #print(final_df.head())

        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
        merged.to_csv(mapping_txt_path, sep=';', index=False, encoding='utf-8')
        print(f"âœ… Î•Î½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿: {mapping_txt_path}")

    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· days mapping: {e}")





#isos vale auto san sinartis gia elegxo kathimerinon
def write_invalid_days_log(invalid_log_lines, output_dir):
    """
    Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Ï„Î¹Ï‚ Î¼Î· Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ OPERATINGDAY ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ txt.
    """
    if not invalid_log_lines:
        return

    log_file_path = os.path.join(output_dir, "invalid_operating_days_log.txt")
    with open(log_file_path, 'w', encoding='utf-8') as f:
        for line in invalid_log_lines:
            if isinstance(line, str) and line.strip():
                f.write(line + '\n')

    print(f"ğŸ“„ ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†Î·ÎºÎ±Î½ {len(invalid_log_lines)} Î¼Î· Î­Î³ÎºÏ…ÏÎµÏ‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ ÏƒÏ„Î¿ {log_file_path}")



def create_depot_timetables(combined_output_path):
    timetable = pd.read_csv(combined_output_path, sep=';', dtype=str)
    patternsA_df = pd.read_csv('static/PatternAttributes.csv', sep=';', dtype=str)
    depots = pd.read_csv("static/DEPOTS.txt", sep=';', dtype=str)

    #print(patternsA_df.head())
    #print(patternsA_df.columns.tolist())
    #print(depots.head())

    # Merge the dataframes on PATTERNCODE
    # This will bring the DEPOT column from patternsA_df into timetable
    timetable = timetable.merge(
        patternsA_df[['PATTERNCODE', 'Depot']],
        how='left',
        right_on='PATTERNCODE',
        left_on='PATTERN'
    )

    #print(timetable.head())

    timetable = timetable.merge(
        depots[['wrong_depot','correct_depot']],
        how='left',
        right_on='wrong_depot',
        left_on='Depot'
    )

    # Assign Depot to ProjectDepot
    #timetable['ProjectDepot'] = timetable['Depot']

    #drop Depot collumn
    timetable = timetable.drop(columns=['Depot'])
    timetable = timetable.drop(columns=['wrong_depot'])

    timetable.to_csv("output/combined_all_timetables.csv", sep=';', index=False, encoding='utf-8')


def after_insert_day_type():
    combined_csv_path = 'output/combined_all_timetables.csv',
    day_maps= "output/days_mapping.txt"
    day_maps = pd.read_csv(day_maps, sep=';', dtype=str)



    # Convert the Day_Map column from string to list
    day_maps['DayType'] = day_maps['DayType'].apply(
        lambda x: ast.literal_eval("[" + ",".join(f'"{i.strip()}"' for i in x.strip("[]").split(",")) + "]")
    )

    # Explode the list into multiple rows
    result = day_maps.explode('DayType')

    # Optional: reset index
    result = result.reset_index(drop=True)

    print(result)
    result.to_csv('output/expanded_days_mapping.csv', sep=';', index=False, encoding='utf-8')


# === ÎšÎ»Î®ÏƒÎ· ÎºÏÏÎ¹Î¿Ï… Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚ ===
if __name__ == '__main__':


    #clean_timetable_dir()
    #timetable_and_combine()



    #-----createdepot sthlh telos timetable.csv apo patternAttributes
    #create_depot_timetables(combined_output_path='output/combined_all_timetables.csv')




    #create_days_mapping_file( combined_csv_path='output/combined_all_timetables.csv',output_txt_path='output/days_mapping.txt')


    #-----------------------------------------------------------------------------------------------------------------------------------------------------


    #-----auto meta to maior kai daytype------#
    #after_insert_day_type()


    #update_days_mapping_from_static(mapping_txt_path='output/days_mapping.txt',static_csv_path='static/Operating_Days.csv')

    #clean_blocks_dir()
    #blocks_and_mergedDepotDaytype()

    clean_duties_dir()
    setup_log()
    duties()
    final_duties()



